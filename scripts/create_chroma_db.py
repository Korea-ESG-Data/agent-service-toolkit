import os
import re
import shutil

from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader
from langchain_openai import OpenAIEmbeddings

# Load environment variables from the .env file
load_dotenv()


def extract_gri_standard_number(filename: str) -> str | None:
    """íŒŒì¼ëª…ì—ì„œ GRI í‘œì¤€ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    ì˜ˆ: "GRI 101_ Biodiversity 2024 - English.pdf" -> "GRI 101"
        "GRI 2_ General Disclosures 2021.pdf" -> "GRI 2"
        "GRI 11_ Oil and Gas Sector 2021.pdf" -> "GRI 11"
    """
    pattern = r"GRI\s+(\d+[A-Z]?)\s*[_:]"
    match = re.search(pattern, filename, re.IGNORECASE)
    if match:
        return f"GRI {match.group(1)}"
    return None


def create_chroma_db(
    folder_path: str,
    db_name: str = "./chroma_db",
    delete_chroma_db: bool = True,
    chunk_size: int = 2000,
    overlap: int = 500,
    recursive: bool = True,
    extra_metadata: dict[str, str] | None = None,
    collection_name: str | None = None,
):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        folder_path: ë¬¸ì„œê°€ ìˆëŠ” í´ë” ê²½ë¡œ
        db_name: ë²¡í„° DB ì €ì¥ ê²½ë¡œ (ê¸°ë³¸ê°’: "./chroma_db")
        delete_chroma_db: ê¸°ì¡´ DB ì‚­ì œ ì—¬ë¶€
        chunk_size: ì²­í¬ í¬ê¸°
        overlap: ì²­í¬ ê²¹ì¹¨ í¬ê¸°
        recursive: í•˜ìœ„ í´ë” ì¬ê·€ íƒìƒ‰ ì—¬ë¶€
        extra_metadata: ì¶”ê°€í•  ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (ëª¨ë“  ì²­í¬ì— ì ìš©)
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„ (Noneì´ë©´ ê¸°ë³¸ ì»¬ë ‰ì…˜ ì‚¬ìš©)
    """
    embeddings = OpenAIEmbeddings(api_key=os.environ["OPENAI_API_KEY"])

    # Initialize Chroma vector store
    if delete_chroma_db and os.path.exists(db_name):
        shutil.rmtree(db_name)
        print(f"Deleted existing database at {db_name}")

    # ChromaëŠ” persist_directoryê°€ ìˆìœ¼ë©´ ìë™ìœ¼ë¡œ ê¸°ì¡´ ì»¬ë ‰ì…˜ì„ ë¡œë“œí•©ë‹ˆë‹¤
    # collection_nameì„ ì§€ì •í•˜ë©´ í•´ë‹¹ ì»¬ë ‰ì…˜ì„ ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ ê¸°ë³¸ ì»¬ë ‰ì…˜ ì‚¬ìš©
    # ê°™ì€ DB ë‚´ì—ì„œ ë‹¤ë¥¸ ì»¬ë ‰ì…˜ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë¶„ë¦¬ ì €ì¥ ê°€ëŠ¥
    
    if not delete_chroma_db and os.path.exists(db_name):
        # ê¸°ì¡´ DBê°€ ìˆìœ¼ë©´ ì§€ì •ëœ ì»¬ë ‰ì…˜ì— ì¶”ê°€ (ë˜ëŠ” ê¸°ë³¸ ì»¬ë ‰ì…˜)
        if collection_name:
            print(f"Adding documents to existing collection '{collection_name}' in database at {db_name}")
        else:
            print(f"Adding documents to existing database at {db_name}")
        chroma = Chroma(
            embedding_function=embeddings,
            persist_directory=db_name,
            collection_name=collection_name,
        )
    else:
        # ìƒˆ DB ìƒì„± (ë˜ëŠ” ê¸°ì¡´ DBê°€ ì—†ì„ ë•Œ)
        if collection_name:
            print(f"Creating new collection '{collection_name}' in database at {db_name}")
        else:
            print(f"Creating new database at {db_name}")
        chroma = Chroma(
            embedding_function=embeddings,
            persist_directory=db_name,
            collection_name=collection_name,
        )

    # Initialize text splitter
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)

    # Get all files to process
    files_to_process = []
    if recursive:
        # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í•˜ìœ„ í´ë” íƒìƒ‰
        for root, dirs, filenames in os.walk(folder_path):
            for filename in filenames:
                if filename.endswith((".pdf", ".docx")):
                    file_path = os.path.join(root, filename)
                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
                    rel_path = os.path.relpath(file_path, folder_path)
                    # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ (ì²« ë²ˆì§¸ í•˜ìœ„ í´ë”ëª…)
                    category = rel_path.split(os.sep)[0] if os.sep in rel_path else None
                    files_to_process.append((file_path, filename, category))
    else:
        # ë‹¨ì¼ í´ë”ë§Œ ì²˜ë¦¬
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if os.path.isfile(file_path) and filename.endswith((".pdf", ".docx")):
                files_to_process.append((file_path, filename, None))

    # Process each file
    total_files = len(files_to_process)
    print(f"Found {total_files} files to process.")

    for idx, (file_path, filename, category) in enumerate(files_to_process, 1):
        print(f"\n[{idx}/{total_files}] Processing: {filename}")

        # Load document based on file extension
        if filename.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        elif filename.endswith(".docx"):
            loader = Docx2txtLoader(file_path)
        else:
            continue

        # Load and split document into chunks
        try:
            document = loader.load()
            chunks = text_splitter.split_documents(document)
        except Exception as e:
            print(f"Error loading {filename}: {e}")
            continue

        # Extract GRI standard number from filename
        gri_standard = extract_gri_standard_number(filename)

        # Add metadata to each chunk
        for chunk in chunks:
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ìœ ì§€
            if not chunk.metadata:
                chunk.metadata = {}
            
            # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
            if category:
                chunk.metadata["category"] = category
            
            # GRI í‘œì¤€ ë²ˆí˜¸ ì¶”ê°€
            if gri_standard:
                chunk.metadata["gri_standard"] = gri_standard
            
            # ì›ë³¸ íŒŒì¼ëª… ëª…ì‹œì  ì¶”ê°€
            chunk.metadata["filename"] = filename
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ ëª¨ë“  ì²­í¬ì— ì ìš©
            if extra_metadata:
                chunk.metadata.update(extra_metadata)

        # Add chunks to Chroma vector store
        try:
            # ì§€ì •ëœ ì»¬ë ‰ì…˜ì— ë¬¸ì„œ ì¶”ê°€
            collection_info = f"collection: {collection_name}" if collection_name else "default collection"
            chroma.add_documents(chunks)
            print(f"  âœ“ Added {len(chunks)} chunks to database ({collection_info})")
        except Exception as e:
            print(f"  âœ— Error adding chunks: {e}")
            raise  # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œì„œ í˜¸ì¶œìê°€ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•¨
    
    # ChromaëŠ” ìë™ìœ¼ë¡œ persistë˜ì§€ë§Œ, ëª…ì‹œì ìœ¼ë¡œ persistí•˜ì—¬ ì €ì¥ ë³´ì¥
    # persist_directoryë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ ì €ì¥ë˜ë¯€ë¡œ ì¶”ê°€ ì‘ì—… ë¶ˆí•„ìš”
    
    print(f"\nâœ“ Vector database created and saved in {db_name}.")
    print(f"  Total files processed: {total_files}")
    if collection_name:
        print(f"  Collection: {collection_name}")
    return chroma


if __name__ == "__main__":
    # Path to the folder containing the documents
    folder_path = "./data"
    db_name = "./chroma_db"

    # Create the Chroma database
    chroma = create_chroma_db(
        folder_path=folder_path,
        db_name=db_name,
        recursive=True,
    )

    # Create retriever from the Chroma database
    retriever = chroma.as_retriever(search_kwargs={"k": 3})

    # Perform a similarity search
    query = "What's my company's mission and values"
    similar_docs = retriever.invoke(query)

    # Display results
    for i, doc in enumerate(similar_docs, start=1):
        print(f"\nğŸ”¹ Result {i}:")
        print(f"Content: {doc.page_content[:200]}...")
        print(f"Metadata: {doc.metadata}")
